//
//  ImageCaptureCoordinator.swift
//  Sparks Gen Counter
//
//  Created by Jonas Hoang on 5/7/25.
//

import Foundation
import Combine
import Vision
import SwiftUI
import AppKit
import ScreenCaptureKit
import AVFoundation

/// macOS-only typealias
typealias PlatformImage = NSImage

class ImageCaptureCoordinator: ObservableObject {
    private var isCapturing = false
    private var cancellables = Set<AnyCancellable>()

    /// üìå Danh s√°ch t·∫•t c·∫£ card trong app (truy·ªÅn v√†o l√∫c init)
    private let allCards: [CardData]

    init(allCards: [CardData]) {
        self.allCards = allCards

        // L·∫Øng nghe notification cardDetected
        NotificationCenter.default.publisher(for: .cardDetected)
            .compactMap { $0.object as? CardData }
            .sink { card in
                print("[OCR] üé¥ Nh·∫≠n di·ªán ƒë∆∞·ª£c th·∫ª: \(card.name)")
            }
            .store(in: &cancellables)
    }

    // MARK: - Start capture overlay

    func beginCaptureOnce() {
        guard !isCapturing else {
            print("[Capture] üö´ ƒêang capture r·ªìi.")
            return
        }

        isCapturing = true
        print("[Capture] ‚úÖ B·∫Øt ƒë·∫ßu overlay ch·ªçn v√πng ·∫£nh...")

        let overlay = SelectionOverlayView(
            onSelectionComplete: { rect in
                Task {
                    await self.captureAndProcess(rect: rect)
                    await MainActor.run {
                        self.isCapturing = false
                        OverlayWindow.shared.hide()
                    }
                }
            },
            onCancel: {
                print("[Capture] üõë Ng∆∞·ªùi d√πng hu·ª∑ capture.")
                self.isCapturing = false
                OverlayWindow.shared.hide()
            }
        )

        OverlayWindow.shared.show(with: overlay)
    }

    func cancelCapture() {
        print("[Capture] ‚ùóÔ∏è B·ªã hu·ª∑ th·ªß c√¥ng.")
        isCapturing = false
        OverlayWindow.shared.hide()
    }

    // MARK: - New method to actually capture and process the rect
    
    static func captureScreenKit(in rect: CGRect) async -> NSImage? {
        do {
            let content = try await SCShareableContent.excludingDesktopWindows(false, onScreenWindowsOnly: false)
            guard let display = content.displays.first else {
                print("[CaptureKit] ‚ùå Kh√¥ng t√¨m th·∫•y display")
                return nil
            }

            let filter = SCContentFilter(display: display, excludingWindows: [])
            let config = SCStreamConfiguration()
            config.width = Int(rect.width)
            config.height = Int(rect.height)
            config.pixelFormat = kCVPixelFormatType_32BGRA

            let delegate = ScreenCaptureDelegate()
            let stream = SCStream(filter: filter, configuration: config, delegate: delegate)
            try await stream.startCapture()

            let sampleBuffer = try await delegate.waitForFirstFrame()
            try await stream.stopCapture()

            guard let pixelBuffer = sampleBuffer.imageBuffer else {
                print("[CaptureKit] ‚ùå Kh√¥ng c√≥ pixelBuffer")
                return nil
            }

            let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            let rep = NSCIImageRep(ciImage: ciImage)
            let nsImage = NSImage(size: rep.size)
            nsImage.addRepresentation(rep)

            return nsImage

        } catch {
            print("[CaptureKit] ‚ùå L·ªói: \(error)")
            return nil
        }
    }


    final class ScreenCaptureDelegate: NSObject, SCStreamDelegate {
        private var continuation: CheckedContinuation<CMSampleBuffer, Error>?

        func waitForFirstFrame() async throws -> CMSampleBuffer {
            try await withCheckedThrowingContinuation { cont in
                self.continuation = cont
            }
        }

        func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of outputType: SCStreamOutputType) {
            if let cont = continuation {
                continuation = nil
                cont.resume(returning: sampleBuffer)
            }
        }
    }


    func captureAndProcess(rect: CGRect) async {
        print("[Capture] üéØ ƒêang ch·ª•p v√πng: \(rect)")

        if let image = await Self.captureScreenKit(in: rect) {
            print("[Capture] ‚úÖ Capture th√†nh c√¥ng, b·∫Øt ƒë·∫ßu OCR")
            self.processImage(image)
        } else {
            print("[Capture] ‚ùå Kh√¥ng capture ƒë∆∞·ª£c ·∫£nh t·ª´ v√πng ch·ªçn.")
        }
    }
    // MARK: - X·ª≠ l√Ω OCR ch√≠nh

    func processImage(_ image: NSImage) {
        guard let cgImage = image.cgImageForOCR() else {
            print("[OCR] ‚ùå ·∫¢nh kh√¥ng h·ª£p l·ªá (kh√¥ng t·∫°o CGImage ƒë∆∞·ª£c)")
            return
        }

        print("[OCR] üñºÔ∏è B·∫Øt ƒë·∫ßu x·ª≠ l√Ω OCR...")

        let request = VNRecognizeTextRequest { [weak self] request, error in
            guard let self = self else { return }

            if let error = error {
                print("[OCR] ‚ùóÔ∏è L·ªói: \(error.localizedDescription)")
                return
            }

            let observations = request.results as? [VNRecognizedTextObservation] ?? []
            let lines = observations.compactMap { $0.topCandidates(1).first?.string }

            print("[OCR] ‚úÖ K·∫øt qu·∫£ text:")
            for line in lines {
                print("üëâ \(line)")
            }

            self.parseTextLines(lines)
        }

        request.recognitionLevel = .accurate
        request.usesLanguageCorrection = true

        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

        DispatchQueue.global(qos: .userInitiated).async {
            do {
                try handler.perform([request])
            } catch {
                print("[OCR] ‚ùóÔ∏è VNImageRequestHandler error: \(error)")
            }
        }
    }

    // MARK: - Parse lines ƒë·ªÉ nh·∫≠n di·ªán th·∫ª

    private func parseTextLines(_ lines: [String]) {
        for line in lines {
            let lower = line.lowercased()

            // CASE 1: Summoned (creature)
            if lower.contains("summoned") {
                if let matchName = extractCardName(from: line, keyword: "summoned") {
                    if let card = allCards.first(where: { $0.name.lowercased() == matchName.lowercased() && $0.type == .creature }) {
                        print("[OCR] üéØ T√¨m th·∫•y Creature: \(card.name)")
                        NotificationCenter.default.post(name: .cardDetected, object: card)
                        return
                    }
                }
            }

            // CASE 2: Spell (‚úß)
            if lower.contains("‚úß") {
                if let matchName = extractCardName(from: line, keyword: "‚úß") {
                    if let card = allCards.first(where: { $0.name.lowercased() == matchName.lowercased() && $0.type == .spell }) {
                        print("[OCR] üéØ T√¨m th·∫•y Spell: \(card.name)")
                        NotificationCenter.default.post(name: .cardDetected, object: card)
                        return
                    }
                }
            }
        }

        print("[OCR] ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y card ph√π h·ª£p trong text.")
    }

    /// Helper: c·∫Øt t√™n th·∫ª sau keyword
    private func extractCardName(from line: String, keyword: String) -> String? {
        guard let range = line.range(of: keyword, options: .caseInsensitive) else {
            return nil
        }

        let after = line[range.upperBound...].trimmingCharacters(in: .whitespacesAndNewlines)
        print("[OCR] ‚úÇÔ∏è Extracted sau '\(keyword)': \(after)")
        return after
    }
}
